\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Recurrent learning}{2}{subsection.2.1}}
\newlabel{expansion}{{4}{2}{Recurrent learning}{equation.2.4}{}}
\newlabel{expanded}{{5}{2}{Recurrent learning}{equation.2.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Back-propagation through time}}{3}{algorithm.1}}
\newlabel{alg:cap}{{1}{3}{Recurrent learning}{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Real-time recurrent learning}}{3}{algorithm.2}}
\newlabel{alg:cap}{{2}{3}{Recurrent learning}{algorithm.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces TODO: Make the caption concise. Three structures of recurrent neural networks that can be trained without truncation. Recurent networks with a columnar structure can be trained end-to-end using gradients without any truncation, only requiring $O(n)$ operations and memory per step. However, columnar networks do not have hierarchical recurrent features---recurrent features made out of other recurrent features. Constructive networks have hierarchical recurrent features, however must be trained incrementally to prevent bias. Incremental learning is achieved by initializing all $w_i$ to zero, and learning $h_1$, $h_2$, and $h_3$ in order. Finally, columnar and constructive networks can be combined to get a hybrid network. The pairs $(h_1, h_2)$ and $(h_3, h_4)$ do not depend on each other, and can learn in parallel. Hoever, $(h_3, h_4)$ must be learned after $(h_1, h_2)$ have been learned and fixed.}}{4}{figure.1}}
\citation{Rafiee2020TestbedsFR}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}General value functions and }{5}{subsection.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Problem Formulation}{5}{section.3}}
\newlabel{eq:return}{{6}{5}{General value functions and}{equation.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Animal Learning Benchmarks}{5}{subsection.3.1}}
\newlabel{animal_bench}{{3.1}{5}{Animal Learning Benchmarks}{subsection.3.1}{}}
\newlabel{animal_results}{{6}{6}{Animal Learning Benchmarks}{section.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Left: Comparison of the predictions produced by the different methods for the last 400 timesteps of training. Right: Performance when an LSTM is trained using the proposed and other online baseline methods on the trace patterning benchmark (lower is better). All the runs are averaged using 30 seeds, and the error regions are the 95\% confidence intervals. T-BPTT uses a truncation length k=27. We can see from this plot that the Diagonal Jacobian method performs the worst while the hybrid networks achieve the lowest error. TODO: T-BPTT is only 15 runs while the remaining curves are 30 runs. TODO: Maybe hybrid networks should get a better name so that people can refer to it easily?}}{6}{figure.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Divergence for truncated-backprop and Snap-1}{6}{section.4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Proposed methods}{6}{section.5}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Empirical evaluation}{7}{section.6}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Temporally symmetric hierarchical recurrent learning using generate and test}{7}{section.7}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusions}{8}{section.8}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Hyper-parameter Settings}{9}{appendix.A}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Sweep ranges for each hyper-parameter. TODO: The notation for last two rows is just placeholder. Replace it with something appropriate.}}{9}{table.1}}
\newlabel{tab:sweep-ranges}{{1}{9}{Sweep ranges for each hyper-parameter. TODO: The notation for last two rows is just placeholder. Replace it with something appropriate}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The best hyper-parameter configuration found for each method according to the average final error over multiple runs. Note that although we specify the $n$ parameter for T-BPTT, Diagonal Jacobian and Columnar approaches, no new feature will be added during their training. It is simply the width of the network. }}{9}{table.2}}
\newlabel{tab:best-params}{{2}{9}{The best hyper-parameter configuration found for each method according to the average final error over multiple runs. Note that although we specify the $n$ parameter for T-BPTT, Diagonal Jacobian and Columnar approaches, no new feature will be added during their training. It is simply the width of the network}{table.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Implementation Details}{9}{appendix.B}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Forward-mode gradient computation of an LSTM cell }{10}{appendix.C}}
\newlabel{i}{{8}{10}{Appendix B}{equation.C.8}{}}
\newlabel{f}{{9}{10}{Appendix B}{equation.C.9}{}}
\newlabel{o}{{10}{10}{Appendix B}{equation.C.10}{}}
\newlabel{g}{{11}{10}{Appendix B}{equation.C.11}{}}
\newlabel{c}{{12}{10}{Appendix B}{equation.C.12}{}}
\newlabel{state_update}{{13}{10}{Appendix B}{equation.C.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}$\frac  {\partial h(t)}{\partial W_{i}}$}{10}{subsection.C.1}}
\newlabel{def1}{{14}{10}{$\frac {\partial h(t)}{\partial W_{i}}$}{equation.C.14}{}}
\newlabel{def2}{{15}{10}{$\frac {\partial h(t)}{\partial W_{i}}$}{equation.C.15}{}}
\newlabel{def3}{{16}{10}{$\frac {\partial h(t)}{\partial W_{i}}$}{equation.C.16}{}}
\newlabel{def4}{{17}{10}{$\frac {\partial h(t)}{\partial W_{i}}$}{equation.C.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}$\frac  {\partial h(t)}{\partial u_i}$}{12}{subsection.C.2}}
\newlabel{thu}{{18}{12}{$\frac {\partial h(t)}{\partial u_i}$}{equation.C.18}{}}
\newlabel{tcu}{{20}{12}{$\frac {\partial h(t)}{\partial u_i}$}{equation.C.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}$\frac  {\partial h(t)}{\partial b_i}$}{13}{subsection.C.3}}
\newlabel{thb}{{22}{13}{$\frac {\partial h(t)}{\partial b_i}$}{equation.C.22}{}}
\newlabel{tcb}{{24}{13}{$\frac {\partial h(t)}{\partial b_i}$}{equation.C.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.4}$\frac  {\partial h(t)}{\partial W_{f_j}}$}{14}{subsection.C.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.5}$\frac  {\partial h(t)}{\partial W_{o_j}}$}{15}{subsection.C.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.6}$\frac  {\partial h(t)}{\partial W_{g_j}}$}{15}{subsection.C.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.7}$\frac  {\partial h(t)}{\partial u_{o}}$}{16}{subsection.C.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.8}$\frac  {\partial h(t)}{\partial u_{f}}$}{16}{subsection.C.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.9}$\frac  {\partial h(t)}{\partial u_{g}}$}{17}{subsection.C.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.10}$\frac  {\partial h(t)}{\partial b_{g}}$}{17}{subsection.C.10}}
\bibdata{sample}
\bibcite{Rafiee2020TestbedsFR}{{1}{2020}{{Rafiee}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.11}$\frac  {\partial h(t)}{\partial b_{f}}$}{18}{subsection.C.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.12}$\frac  {\partial h(t)}{\partial b_{o}}$}{18}{subsection.C.12}}
