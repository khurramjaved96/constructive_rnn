%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amsmath,amsfonts,amssymb,mathtools}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{hyperref}
\allowdisplaybreaks
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2021}


\newcommand{\etal}{\textit{et al}.}
\newcommand{\ie}{\textit{i}.\textit{e}., }
\newcommand{\eg}{\textit{e}.\textit{g}. }
\newcommand{\algoname}{\textit{Master-User} }
\newcommand{\archi}{\textit{Col-NN} }

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2021}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Columnar Networks for Recurrent Learning}

\begin{document}

\twocolumn[
\icmltitle{Scalable Online Recurrent Learning Using \\ Columnar Neural Networks}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2021
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aeiau Zzzz}{equal,to}
\icmlauthor{Bauiu C.~Yyyy}{equal,to,goo}
\icmlauthor{Cieua Vvvvv}{goo}
\icmlauthor{Iaesut Saoeu}{ed}
\icmlauthor{Fiuea Rrrr}{to}
\icmlauthor{Tateu H.~Yasehe}{ed,to,goo}
\icmlauthor{Aaoeu Iasoh}{goo}
\icmlauthor{Buiui Eueu}{ed}
\icmlauthor{Aeuia Zzzz}{ed}
\icmlauthor{Bieea C.~Yyyy}{to,goo}
\icmlauthor{Teoau Xxxx}{ed}
\icmlauthor{Eee Pppp}{ed}
\end{icmlauthorlist}

\icmlaffiliation{to}{Department of Computation, University of Torontoland, Torontoland, Canada}
\icmlaffiliation{goo}{Googol ShallowMind, New London, Michigan, USA}
\icmlaffiliation{ed}{School of Computation, University of Edenborrow, Edenborrow, United Kingdom}

\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}
\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

Learning using neural networks requires structural credit-assignment --- identifying parameters that influence a prediction made by the network. For recurrent learning, a parameter can influence a prediction made many steps in the future making credit-assignment challenging. Two popular gradient-based algorithms for structural credit-assignment in recurrent networks are (1) Back-propagation Through Time (BPTT) and (2) Real-time Recurrent Learning (RTRL). BPTT requires memory proportional to the length of the input sequence and scales poorly. Additionally, it does not spread the operations for gradient computation uniformly across time. RTRL, on the other hand, can compute gradients in real-time for arbitrarily long sequences using constant memory but is computationally intractable for large networks. In this work, we propose a network architecture and an approximation to RTRL that allow us to approximate gradients in real-time using $O(n)$ operations and memory per-step. Our method builds on the idea that for modular recurrent networks composed of columns with scalar states --- called Columnar Neural Networks --- it is sufficient for a parameter to track its influence on the state of its own column.  As long as connections across columns are sparse compared to connections with-in a column, our approximation is close to the true gradient. Our architecture can be applied for learning recurrent states and for meta-learning by using a single trace per-parameter.
\end{abstract}

\section{Introduction}
Structural credit-assignment --- identifying how to change network parameters to improve predictions --- is an essential component for learning distributed representations using neural network. Effective structural credit-assignment requires tracking the influence of parameters on future predictions. A parameter can influence a prediction in the future in two primary ways. First, for recurrent networks, the parameter can influence the internal state of the network which, in turn, can affect the prediction made many steps in the future. Second, if the network is learning online, the parameter can influence the learning updates. These learning updates, in turn, influence the predictions made in the future. Structural credit-assignment through recurrent states is called recurrent state learning, whereas through the learning updates is called meta-learning (Bengio, 1990; Sutton, 1992). 

A popular algorithm for gradient-based structural credit-assignment in recurrent networks is Back-Propagation Through Time (BPTT)~(Werbos, 1988; Robinson and Fallside, 1987). BPTT extends the back-propagation algorithm for feed-forward networks --- independently proposed by Werbos~(1974) and Rumelhart~(1986) --- to recurrent networks by storing network activations from prior steps, and repeatedly applying chain-rule starting from the output of the network and ending at the activations at the beginning of the sequence. While BPTT was proposed to train recurrent networks with an internal state, it can also be used for meta-learning. Finn~\etal~(2017) and Li~\etal~(2017) independently proposed using BPTT for learning through a stochastic gradient descent (SGD) update. This is possible because an SGD update recursively updates the parameters of the network, and for a twice-differentiable loss function, the update itself is differentiable. As a result, an online learning feed-forward network can be seen as a recurrent network with a recurrence relationship defined by the weight updates. The main limitation of BPTT is that it requires memory and operations proportional to the length of the sequence. For online learning, this sequence can be never-ending or arbitrarily long. 

% \begin{figure*}
% \centering
%   \includegraphics[width=0.8\textwidth]{columnar_networks_paper/figures/ColumnarNetwork.pdf}
%   \caption{Architecture for Columnar Neural Network (\archi). A \archi has an n-dimensional hidden state --- one outputted by each column. The parameters in column $i$ are $\theta_i$ and they only keep track of their influence on $h_i$ over time. Column $j$ feature extractor is free to use features from column $i$ using lateral connections. However, $\theta_i$ ignores how they influence $h_j$. Increasing the size of a \archi by adding more columns or increasing the size of each column only linearly increases the memory and operations per-step for structural credit-assignment.}
%   \label{cnn}
% \end{figure*}
The dependence of BPTT on storing activations from past hinders its application to online learning. Additionally, the computation in BPTT is not spread uniformly across time --- the learner accumulates activations for a sequence in the memory and delays the computation of the gradients until the end. This is incompatible with the goal of real-time learning --- the ability to incorporate feedback from the environment quickly for correcting mistakes. 


RTRL --- an alternative to BPTT --- was independently proposed by Robinson~and~Fallside (1987) and Williams~and~Zepser (1989). RTRL relies on forward-mode differentiation --- using chain-rule to compute gradients in the direction of time --- to compute gradients recursively. Unlike BPTT, RTRL does not delay gradient-computation until the final step. The memory requirement of RTRL also does not depend on the sequence length. This makes it a better candidate for real-time online learning. Unfortunately, RTRL requires maintaining the jacobian $\frac{\partial h(t)}{\partial \theta}$ at every step, which requires $O(|h||\theta|)$ memory, where $|h|$ is the size of state of the network and $|\theta|$ is the number of total parameters. The jacobian is recursively updated by applying chain rule as:  
$$\frac{\partial h(t+1)}{\partial \theta} =\frac{\partial h(t+1)}{\partial \theta(t)} +  \frac{\partial h(t+1)}{\partial h(t)}\frac{\partial h(t)}{\partial \theta}, $$ 
which requires  $O(|h|^2|\theta|)$ operations. As a result, the amount of resources required to apply RTRL to even moderately sized networks is intractable.

RTRL can also be used for meta-learning. Sutton~(1992) showed that it can be used to learn the step-sizes of parameters of an online learning linear predictor. Veeriah~\etal~(2017) extended Sutton's analysis by showing that RTRL can also be used to approximate meta-gradients for the parameters of a single hidden layer neural network. Directly applying RTRL to the deep-learning-based meta-learning methods, however, is not tractable. MAML~(Finn~2017) --- a popular meta-learning algorithm that updates all parameters online at each learning step --- would require $O(|\theta|^2)$ memory and $O(|\theta|^3)$ operations per-step for recursively computing the meta-gradient. An alternative to MAML is the OML architecture --- independently proposed by Javed and White~(2019), and Bengio~\etal~(2020). The OML architecture updates only the parameters in the final prediction layers --- $W$ --- of the network at every step and updates the remaining parameters --- $\theta$ --- using the meta-gradient. Raghu~(2019) showed that the more restricted OML architecture is competitive with MAML on multiple benchmarks. Computing meta-gradients for the OML architecture using RTRL requires $O(|W||\theta|)$ memory and $O(|W^2||\theta|)$ operations per-step. For $|W| << |\theta|$, this is already significantly more tractable than the MAML architecture. However, the memory and compute still does not scale linearly with the size of the network. For a more thorough explanation of the memory and time complexity of recurrent state-learning and meta-learning methods using RTRL, see Appendix A. 
 
 
% \begin{figure*}
% \centering
%   \includegraphics[width=0.80\textwidth]{columnar_networks_paper/figures/lateral_connections.pdf}
%   \caption{We can divide $\theta$ into $\{\theta_1, \theta_2\}$ randomly, or in columns. Our goal is to estimate $\frac{\partial h_i}{\partial \theta_i}$ for $i=1$ and $2$. For the network on the left, this would require two backward passes --- one starting from $h_1$ and one from $h_2$. For the Columnar network on the right, on the other hand, we can back-propagate gradients starting from $h_1 + h_2$ and stop the flow of gradients from parameters connecting the two columns, shown with $\times$ on the arrows. This allows us to compute $\frac{\partial h_i}{\partial \theta_i}$ for $i=1$ and $2$ in a single backward pass. Organizing parameters in columns, as a result, is key to making the gradient-estimation more tractable.}
%   \label{lateral_connections}
% \end{figure*}

Structural credit-assignment can be done more efficiently by approximating the gradient instead of using the exact gradient. Elman~(1990) proposed to ignore the influence of parameters on future predictions completely for training RNNs. This resulted in a scalable but biased algorithm. His idea was generalized to Truncated-BPTT by practitioners, who proposed tracking the influence of parameters on predictions for only $k$ steps in the future. They achieved this by limiting the BPTT computation to last $k$ activations~(Mikolov~\etal, 2009, 2010; Sutskever, 2013). T-BPTT is far more tractable than BPTT for online learning, but the resultant gradient is blind to long-range dependencies. Tallec~\etal~(2017) showed T-BPTT can cause divergence when a parameter has a negative long-term effect on a target but a positive short-term effect. Ollivier~\etal~(2015) and Tallec~\etal~(2017) addressed this bias by proposing NoBacktrack and UORO, respectively. Both of these algorithms provide real-time unbiased estimates of the gradient and scale well to large networks and sequences. However, their estimates are stochastic and have high variance, requiring extremely small learning-rates for effective learning. Cooijmans and Martens~(2019) and Menick~\etal~(2021) showed that for practical problems, UORO does not perform well due to its high variance compared to other approximations.

 Menick~\etal~(2021) proposed their own approximation to RTRL, called SnAp-$k$. SnAp tracks the influence of a parameter on a state only if the parameter can influence the state in $k$ steps. The gradient estimated by SnAp-$k$ differs from T-BPTT with a truncation window of $k$ significantly. SnAp identifies parameters whose influence on a parameter is zero for $k$ steps, and assume the future influence to be zero as well. For the remaining parameters, it tracks their influence on the state for all future steps, similar to RTRL. T-BPTT, on the other hand, ignores the influence of all parameters on all hidden states beyond $k$ steps. As a result, SnAp-k is less biased than T-BPTT. SnAp-1 is computationally efficient but introduces significant bias. SnAp-$k$ for $k>1$ reduces bias but can be as expensive as RTRL for dense RNNs.  Menick~\etal~(2021) further proposed connection sparsity as a way to make SnAp more scalable. SnAp, combined with sparsity, is a promising research direction. However, Menick~\etal~(2021) did not provide a way to implement SnAp to realize the efficiency gains promised by the algorithm. Instead, their algorithm relies on running the RNN for $k$ steps and looking at the full jacobian $\frac{\partial h(t)}{\partial \theta}$ to determine which parameters do not influence which states. Because they induce sparsity randomly in their networks, the resultant sparisty in the jacobian is not structured, and is not amenable to efficiency gains using parallel processors --- GPUs. Moreover, for large networks, computing this jacobian even once is not possible.  Finally, even SnAp-1 can be significantly computationally expensive for recurrent networks that use deep feature extractors. For instant, if internal-states operate on a shared representation computed with a deep network with parameters $\theta$, SnAp-1 could require $O(|\theta||h|)$ memory and $O(|\theta||h|^2)$ operations per-step. Our goal is to design an algorithm that requires $O(|\theta| + |h|)$ memory and operations and can easily be parallelized using existing hardware.



We propose an architecture called Columnar Neural Networks (\archi) for recurrent neural networks and an algorithm --- called \algoname --- for structural credit-assignment in \archi. Our algorithm can be seen as an approximation to RTRL that uses $O(n)$ operations and memory per step. For \archi with sparse lateral connections, \algoname estimates credit that is empirically close to the true gradient. Our algorithm can be used to do scalable recurrent state learning and meta-learning. 



% \begin{figure*}
% \centering
%   \includegraphics[width=0.8\textwidth]{columnar_networks_paper/figures/gradient_accuracy.pdf}
%   \caption{Accuracy of the gradient-estimate compared to the true gradient computed using BPTT. x-axis shows the normalized number of lateral connections between columns, where 1 means maximum possible lateral connections and 0 means no lateral connections. Y-axis shows the quality of approximation. The left side plot shows percentage of parameters of which the estimated gradient points in the same direction as the true gradient whereas the plot on the right shows the absolute error in the gradient estimate. Note that the left plot using the log scale for the x-axis. When no of lateral connections is low, the estimate of the gradient is good. As we move towards fully connected networks and lose the structure of the columnar networks, the gradient approximation gets bad. For fully connected networks, 40\% of the parameters have gradients pointing in the opposite direction. This is close to random gradient, which would point in random directions 50\% of the time.}
%   \label{results_rnn}
% \end{figure*}


\section{Background}
Let $\theta \in \mathbb R^{p} $ be the parameters of a recurrent network and $h(t)~\in~\mathbb R^n$ be the hidden state at time $t$. The state $h(t)$ is linearly combined using $w(t) \in \mathbb R^n $ to make a prediction $y(t)$ as:
\begin{equation}
y(t) = \sum_{i=1}^{n} w_i(t)h_i(t) 
\end{equation}


Let $y^*(t)$ be the target at time $t$. Then the error in the prediction is given by:

\begin{equation}
    \delta(t) = y^*(t) - y(t).
\end{equation}
Our goal is to minimize the error 
\begin{equation}
    \mathcal L(t) = \frac{1}{2}\delta^2(t).
\end{equation}
The goal of a gradient-based structural credit-assignment algorithm can be formalized as estimating:
\begin{align}
G(t) &= \frac{\partial \mathcal L(t)}{\partial \theta} \\ 
% &= \frac{\partial \delta^2(t)}{2 \partial \theta} \\ 
% &= \delta(t)\frac{\partial \delta(t)}{\partial \theta}  \\ 
% &= \delta(t)\frac{\partial (y^*(t) - \sum_{i=1}^{n} w_i(t)h_i(t))}{\partial \theta} \\
% &= -\delta(t)\frac{\partial (\sum_{i=1}^{n} w_i(t)h_i(t))}{\partial \theta} 
\end{align}
Here $\theta$ is not indexed by $t$ because we want to estimate the impact of $\theta$ at all prior time-steps on the current state. Since computing this exact gradient using BPTT or RTRL is not scalable for online learning, we aim to approximate it. We propose an algorithm called \algoname.  
% Let $C(t) \in  \mathbb R^{p}$ be the credit assigned to each state. Then, $C(t)$ is a useful approximation of $G(t)$ if they are aligned --- they have the same sign. 
 
 \section{The Master-User Algorithm}
 The central idea behind \algoname is to divide the parameters of an RNN into $n+1$ disjoint groups --- $\theta = \{\theta_1, \theta_2, \cdots, \theta_n, w\}$ --- and assign every group to a scalar hidden state. Let  $\theta_i$ be assigned to $h_i$. Then, parameters in $\theta_i$ track their influence on $h_i(t)$ when computing $\frac{\partial h(t)}{\partial \theta_i}$, while ignoring their influence on $h_j(t)$. We call $h_i$ the master state for $\theta_i$ --- it can pass gradients to change it --- whereas $h_j$ is a user state --- it can use features generated by $\theta_i$, but can not change those features. To keep track of influence on $h_i$, $\theta_i$ maintains a trace of the gradient $\frac{\partial h_i(t)}{\partial \theta_i}$ online. The trace is updated as: 
 \begin{equation}
 \frac{\partial h_i(t)}{\partial \theta_i} =  \frac{\partial h_i(t)}{\partial \theta_i(t)} + \frac{\partial h_i(t)}{\partial h_i(t-1)} \frac{\partial h_i(t-1)}{\partial \theta_i}.
 \label{master_user}
 \end{equation}
 
 Let $k$ be the total number of nodes in the RNN, including the hidden states. Then, $\frac{\partial h_i(t)}{\partial \theta_i(t)}$ and $ \frac{\partial h_i(t)}{\partial h_i(t-1)}$ can be computed in $O(|\theta| + k)$ operations using the back-propagation algorithm. The product $\frac{\partial h_i(t)}{\partial h_i(t-1)} \frac{\partial h_i(t-1)}{\partial \theta_i}$ and the summation in Equation~\ref{master_user} take $O(|\theta_i|)$ operations. Equation~\ref{master_user} has to be repeated for each hidden state, making the overall complexity $O(|h|(|\theta| + k))$ which falls short of our goal of an order $O(|\theta| +k)$ algorithm. Additionally, assuming $\frac{\partial h_j(t)}{\partial \theta_i}$ to be zero for existing dense RNN architecture, such as LSTM~(Hochreiter, 1997) or GRU~(Cho, 2014) is unjustified and the resulting estimate of the gradients is unlikely to be a good approximation. Master-User, alone, neither satisfies the goals of scaling linearly with number of parameters, nor estimates the gradients accurately. In the next section, we introduce an architecture for RNNs that is compatible with deep feature learning, and enables Master-User to satisfy our two goals.
 
 \section{Columnar Neural Networks}
 \algoname divides the parameter into $n$ disjoint groups, but does specify how these groups are constructed. Parameters in $\theta_i$ can be spread all over the network, or assigned randomly. Columnar Neural Networks (\archi) imposes further structure on these groups by organizing them into $n$ columns. All parameters in column $i$ --- $\phi_i$ --- belong to $\theta_i$. The output of the column at time $t$ is a feature vector $f_i(t) \in \mathbb R^k$.  Let $f(t) \in \mathbb R^{nk}$ be a concatenation of all feature vectors, i.e $f = (f_1, f_2, \cdots, f_n)$. Hidden state $h_i$ is computed as: 
 
 \begin{equation}
     h_i(t) = \mathcal R_{u_i}(f(t), h(t-1))
 \end{equation}
 where $\mathcal R_{u_i}$ defines a recursive relationship between $h_i(t)$ and $h(t-1)$. Parameters $u_i \in \theta_i$ and only keep track of their influence on $h_i$. $\mathcal R_{u_i}$ can be used to implement existing recurrent architectures, such as LSTM or GRU. Parameters An abstracted view of \archi is shown in Figure~\ref{cnn}. At-first, \archi appears to be a contrived architecture, but it provides a key benefit from an optimization point of view --- it can reduce the overhead of \algoname from $O(|h||\theta|)$ to $O(|\theta|)$. This is possible because for \archi, the set $\{\frac{\partial h_i(t)}{\partial \theta_i}\}_{i=1}^n$ can be computed in single backward pass as opposed to $|h|$ backward passes of back-propagation. 
 
 \subsection{$O(n)$ Implementation of \algoname for \archi}
 This can be implemented by introducing stop-gradient connections, and computing the gradient $\frac{\partial \sum_i^n h_i(t)}{\partial \theta}$ as explained in Figure~\ref{lateral_connections}. 
 
 First, we derive the update equations for \algoname with no lateral connections. The $ith$ column processes input $x(t)$ as:
  
  \begin{equation*}
        h_i(t) = C_{\theta_i(t)}(x(t), h_i(t-1))
  \end{equation*}
  where $h_i(t)$ is a scalar. The output of all the columns can be combined to get a the internal state of the RNN given by:
 
 \begin{equation*}
     h(t)= \{h_1(t), h_2(t), \cdots, h_n(t)\}.
 \end{equation*}
The internal state, $h(t)$, is combined with the weight vector  $W=\{w_1, w_2, \cdots, w_n\}$ to make a prediction 
\begin{equation*}
    y(t) = \sum_{i=1}^j w_i(t) h_i(t).
\end{equation*}

 

\section{Columnar Networks for Scalable Online Recurrent Learning}
We derive the forward propagation gradients of a single column of a columnar network. We use a simple recurrent cell defined by the following equations:


\begin{align}
i_i(t) &= \sigma( I_{\theta_i}^j (x(t))) \notag \\
f_i(t) &= \sigma( F_{\theta_i}^j (x(t))) \notag \\
h_i(t) &= f_i(t)h_i(t-1) + (1 - f_i(t))i_i(t) \label{state_update}
\end{align}
where $\sigma$ is the sigmoid activation functions. The cell outputs two values at every-step --- $i(t)$ and $f(t)$ --- between 0 and 1. $f(t)$ is the forget gate that controls how much of the old state should be forgotten whereas $i(t)$ is the input gate that decides how much should be added to the state at the current time-step. The proposed architecture can prevent vanishing gradients similar to an LSTM cell (Hochreiter, 1997) --- it can pass gradients back without any decay --- but is not as notationally complex. We also provide a derivation for an LSTM cell based column in the appendix for the interested readers. 


Let $y^*(t)$ be the target at time $t$. Then the error in the prediction is given by:

\begin{equation}
    \delta(t) = (y^*(t) - y(t))
\end{equation}
Let the loss function be: 
\begin{equation}
    \mathcal L(t) = \frac{1}{2}\delta^2(t).
\end{equation}
Then, our goal is to compute the gradient $\frac{\partial L(t)}{\partial \theta_i}$. 
% \begin{equation}
\begin{align}
          \frac{\partial \mathcal L}{ \partial \theta_i} &= \frac{\partial \delta^2(t)}{ 2\partial \theta_i } \notag\\
          &= \delta(t)\frac{\partial(y^*(t) - \sum_{k=1}^{n} w_k(t) h_k(t))}{ \partial \theta_i} \notag \\
          &= - \delta(t)\sum_{k=1}^{n} w_k(t) \frac{\partial  h_k(t)}{ \partial \theta_i}\label{before} \\
          & \textit{Because of the columnar structure,} \notag \\ & \frac{\partial  h_k(t)}{ \partial \theta_i}=0 \textit{ when }i \ne k \implies \label{after}\\
          \frac{\partial \mathcal L}{ \partial \theta_i} &= - \delta(t) w_i(t) \frac{\partial  h_i(t)}{ \partial \theta_i} \notag
    \end{align}
% \end{equation}
The columnar structure simplifies the gradient computation.    
Note that here $\theta_i$ is not indexed by $t$ and is assumed to be constant throughout the sequence $x(1)$ to $x(t)$. We still have to compute $\frac{\partial h_i(t)}{ \partial \theta_i}$.  Let:
% \begin{equation}
\begin{align}
 TH_{\theta_i}(t)  & \vcentcolon = \frac{\partial h_i(t)}{\partial \theta_i}  && \textit{(By definition)} \label{def1} \\
 TH_{\theta_i}(0)  & \vcentcolon = 0  && \textit{(By definition)} \label{def2} 
\end{align}
% \end{equation}
Then, using equation~\ref{state_update}, we can write:
% \begin{equation}
\begin{align*}
  TH_{\theta_i}(t) & = \frac{\partial}{\partial \theta_i} \left ( f(t)h(t-1) + (1-f(t))i(t) \right ) \\ & \textit{which simplies to} \\
   TH_{\theta_i}(t) & = f(t)TH_{\theta_i}(t-1)  + h(t-1)\frac{\partial f(t)}{\partial \theta_i} \\ & - i(t)\frac{\partial f(t)}{\partial \theta_i}  +   (1-f(t))\frac{\partial i(t)}{\partial \theta_i} \\  & \textit{by repeated application of product and chain rule.} 
\end{align*}


Both $\frac{\partial i(t)}{\partial \theta_i}$ and $\frac{\partial f(t)}{\partial \theta_i}$ only depend on the network activations at the current time-step, and can be computed analytically or by using back-propagation. This results in an estimate of the gradient that requires $O(|\theta|)$ memory and compute per step. Note that even if $i(t)$ and $f(t)$ were to depend on $h_i(t-1)$, we could still easily compute the gradient the trace $ TH_{\theta_i}(t-1)$, which is available at time $t$. For a concrete example, see the LSTM derivation in the appendix. 

\paragraph{Lateral connections for more expressive networks}
One limitation of columnar networks, as described in the last section, is that the lack of feature reuse between columns. This can be fixed introducing lateral connections between columns. One issue is that if there is a lateral connection from column $i$ to $j$, $\frac{\partial ( h_j(t)))}{ \partial \theta_i}$ is no longer zero, an assumption used to simply equation~\ref{before} to equation~\ref{after}. Regardless, we can continue to use the update equation with the hypothesis that if the lateral connections are sparse, equation~\ref{after}~$\approx$~equation~\ref{before}. We treat weights from column $i$ to $j$ to be part of $\theta_j$.
\subsection{Evaluating the quality of approximation}
We compare the quality of estimate of the gradient with the true gradients computed using full-BPTT on a 150 length sequence (sequence details for later) and report the results in Figure~\ref{results_rnn}. See caption for the interpretation of the results. 
\subsection{Columnar Networks for Scalable Online Meta-Learning}
In the previous section, we showed that Columnar Networks enable online recurrent learning with $O(|\theta|)$ compute and memory with little to no approximation in the gradients, depending on the sparsity of lateral connections. In this section, we show that the same results extend to gradient-based meta-learning. For simplicity, we use columns made up of feed-forward network, though the analysis can easily be extended to meta-learning with recurrent networks. 

Once again, let $\theta_i$ be the parameters of the $ith$ column, $h_i(t)=C_{\theta_i}(x)$ and $\delta(t)$ be the prediction error as defined earlier. Since our network is now learning online at every-step, we update $w_i$ as:

\begin{align}
    w_i(t+1) &= w_i(t) - \alpha_i \frac{\partial \mathcal L(t)}{\partial w_i} \notag \\
    &= w_i(t) - \alpha_i \frac{\partial \delta^2(t) }{2\partial w_i} \notag \\
    &= w_i(t) - \alpha_i \delta(t) \frac{\partial (y^*(t) - y(t)) }{\partial w_i} \notag \\
    &= w_i(t) + \alpha_i \delta(t) \frac{\partial \sum_{j=1}^n w_j(t)h_j(t) }{\partial w_i} \notag  \\
     &= w_i(t) + \alpha_i \delta(t) h_i(t) \label{lms}
\end{align}
where $a_i$ is the step-size parameter. Equation~\ref{lms} is the standard least mean-squares learning rule. 

To meta-learn the parameters for the columns, our goal is to compute gradients for parameters $\theta_i$ through the update rule w.r.t to $\mathcal L(t)$.

\begin{align}
          \frac{\partial \mathcal L}{ \partial \theta_i} &= \frac{\partial \delta^2(t)}{ 2\partial \theta_i } \notag \\
          &= \delta(t)\frac{\partial(y^*(t) - \sum_{k=1}^{n} w_k(t) h_k(t))}{ \partial \theta_i}  \notag \\
          & \textit{Unlike last section, $\frac{\partial w_k(t)}{\partial \theta_i}\ne0$ for} \notag\\
          & \textit{an online learning network.} \notag \\
          &= - \delta(t)\sum_{k=1}^{n}  \frac{\partial (w_k(t) h_k(t)))}{ \partial \theta_i} \notag \\
          & \textit{Using product rule} \notag \\ 
           &= - \delta(t)\sum_{k=1}^{n} \left (  w_k(t)\frac{\partial  h_k(t))}{ \partial \theta_i} + h_k(t)\frac{\partial  w_k(t))}{ \partial \theta_i}\right) \notag  \\
             & \frac{\partial ( h_k(t)))}{ \partial \theta_i}=0 \textit{ when }i \ne k \implies \label{due_to_columnar}\\
           &= - \delta(t)\left (  w_i(t)\frac{\partial  h_i(t))}{ \partial \theta_i} + \sum_{k=1}^{n}  h_k(t)\frac{\partial  w_k(t))}{ \partial \theta_i} \right) \label{before_approx} \\
           & \theta_i \textit{ only indirectly influences } w_k \textit{ when }i \ne k \notag \\ & \implies \frac{\partial ( w_k(t)))}{ \partial \theta_i}\approx 0  \implies  \notag \\
           \frac{\partial \mathcal L}{ \partial \theta_i} &\approx - \delta(t)\left (  w_i(t)\frac{\partial  h_i(t))}{ \partial \theta_i} +  h_i(t)\frac{\partial  w_i(t))}{ \partial \theta_i} \right) \label{after_approx}
    \end{align}
The observation in Equation~\ref{due_to_columnar} is only true due to the columnar structure of the network. The approximation used to simply Equation~\ref{before_approx} to Equation~\ref{after_approx} is reasonable because due to the columnar structure, $\theta_i$ only indirectly influences $w_k$ when $i\ne k$.  Although this approximation is similar to the one introduced by Sutton~(1992) and Veeriah~\etal~(2017) for linear and one hidden layer networks, it's application to multi-layer network is only reasonable due to the columnar structures. 

We need to compute $\frac{\partial  h_i(t))}{ \partial \theta_i}$ and $\frac{\partial  w_i(t))}{ \partial \theta_i}$ to finish the derivation. For a feed-forward network, $\frac{\partial  h_i(t))}{ \partial \theta_i}$ can directly be computed using the back-propagation algorithm. $\frac{\partial  w_i(t))}{ \partial \theta_i}$ requires more work. Let: 

\begin{align}
 TW_{\theta_i}(t)  & \vcentcolon = \frac{\partial w_i(t)}{\partial \theta_i}  && \textit{(By definition)} \label{defw_1}\\
 TW_{\theta_i}(0)  & \vcentcolon = 0  && \textit{(By definition)} \label{defw_2} 
\end{align}

Then: 
\begin{align}
TW_{\theta_i}(t+1)   & = \frac{\partial w_i(t+1)}{\partial \theta_i}   \notag \\
& \textit{Using Equation~\ref{lms}}  \notag \\
TW_{\theta_i}(t+1)   & = \frac{\partial (w_i(t) + \alpha_i \delta(t) h_i(t))}{\partial \theta_i}  \label{use_lms} \\
&= TW_{\theta_i}(t) +   \alpha_i \delta(t)  \frac{\partial h_i(t)}{\partial \theta_i} \notag \\ 
& + \alpha_i h_i(t)  \frac{\partial \delta(t)}{\partial \theta_i} \notag \\
\end{align}

Once again, $\frac{\partial h_i(t)}{\partial \theta_i}$ can be efficiently computed using back-propagation. Moreover, from Equation~\ref{after_approx}, we know:
\begin{align}
    \frac{\partial \delta(t)}{\partial \theta_i} &\approx   w_i(t)\frac{\partial  h_i(t))}{ \partial \theta_i} +  h_i(t)\frac{\partial  w_i(t))}{ \partial \theta_i} \\
    &\approx w_i(t)\frac{\partial  h_i(t))}{ \partial \theta_i} +  h_i(t) TW_i(t)
\end{align}


Note that for Columnar Networks with no lateral connections, the recurrent gradient estimate is exact whereas for meta-learning, the estimate is approximate even without lateral connections. Nonetheless, the columnar structure still allows us to get rid of computationally expensive terms, as shown in Equation~\ref{due_to_columnar}.
\subsection{Evaluating the quality of approximation for meta-learning}
Similar to last section, we evaluate the quality of the approximation by comparing it to gradients computed using unapproximated BPTT. 
% \s          ubsection{Experiments} 
% For RNN, we keep the network fixed for the whole sequence, and look at the gradients at the end. The meta-learning network, on the other hand, learns the final prediction layer at every step. We show the average error in the gradient as a function of sparsity in figure~?. 

% \subsection{Links to Meta-learning Representations}
% In recent years, meta-learning has been shown to be a promising approach for learning representations. A meta-learning objective optimizes a second order phenomenon to improve learning, such as speed of learning proposed by Finn in 2017.
% \section{Columnar Networks}
% There are three main components of the columnar network. First, each column is self-recurrent to enable learning in partially observable words. Second, the step-sizes of the prediction layer are learned using meta-gradients to enable fast learning. Large learning rates are essential for an agent to quickly adapt to changes in the world; and finally, the representation is learned by minimizing the generalization error and not the empirical error. All three of these ideas have been explored independently and shown to improve performance on a wide variety of tasks, but they have yet to be combined in scalable way. Moreover, recurrent learning and learning representations by minimizing generalisation error has only been explored using back-propagation through time or truncated bptt. The former does not scale to problems involving long sequences, whereas the latter introduces significant bias in the learning update. Columnar networks, on the other hand, can achieve all three goals online using only three traces per parameter. As such, our methods provide significant computational advantages over existing methods, and provide a viable path for large-scale online representation learning in recurrent networks for maximizing generalization. 

\section{Combining Recurrent and Meta-learning}
\subsection{Experiments}
\section{Connections to Neuroscience}

\section{Conclusions} 
Potentials things to mention: 
1. Note that while we use random lateral connections, it might be possible to do far better by finding features from other columns that are useful. For each feature reuse, the gradient approximation degrades which presents a trade-off between feature reuse and gradient-accuracy. 
% \section{Experiments}
% Our proposed Columnar Networks have three components --- meta-learning representations, meta-learning step-sizes, and recurrent connections. We first demonstrate the usefulness of each independently. 
% First, we verify that each of the three components of the columnar networks are important for representation learning --- meta-learning step-sizes of the prediction learning network, meta-learning representations, and recurrent connections.
% \subsection{Step-size adaptation for fixed representations}
% Adapting step-sizes for a set for a set of features is a rudimentary from of representation learning. An agent can learn to ignore an irrelevant feature by setting its step-size and weight to be zero. Alternatively, a step-size allows an agent to use a feature for effective tracking.  setting its step-size to be large. Fortunately, for linear predictions, Sutton (1992) proposed an incremental algorithm for learning step-sizes for tracking a non-stationary problem. Since then, many heuristic-based algorithms have been popularized for adapting step-sizes such as Adam optimizer (Kingma 2014), RMSProp  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DELETE THIS PART. DO NOT PLACE CONTENT AFTER THE REFERENCES!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Step-size Adaptation Derivation}
% \section{Representation update gradients}
% Let $x(t)$ be the input vector at time $t$. $c_{\theta_i}$ is the $ith$ column of the network and is parameterized by $\theta_i$. It outputs a scalar value $h_i(t)=c_{\theta_i}(x(t))$. The output of all the columns, $h_1, h_2, \cdots , h_n$ is linearly combined using weights $w_1, w_2, \cdots, w_n$ to make the final prediction $y$. The ground truth target is denoted by $y^{*}$. The loss at time-step $t$ is given by: 
% \begin{equation}
% \begin{aligned}
%     \mathcal L(t) &=  \frac{1}{2}(y^*(t) - \sum_{i=1}^n w_i(t) h_i(t))^2 \\
%     &= \frac{1}{2}(y^*(t) - y(t))^2 
%     \end{aligned}
% \end{equation}
% The goal of the agent is to minimize online accumulated error --- proportional to online regret ---  given by: 
% \begin{equation}
% \begin{aligned}
%     \mathcal G(t) &= \sum_{j=1}^t \mathcal L(j) \\
%     &= \sum_{j=1}^t \frac{1}{2}(y^*(j) - y(j))^2 .
% \end{aligned}
% \end{equation}
% Normally in neural network training, the representation parameters $\theta$ are updated by minimizing the loss $\mathcal L(t)$ using gradient-descent. The gradients are computed using the back-propagation algorithm (Werbos 1974, Rumelhart 1986, and Lecun 1985) to update $\theta(t)$ to $\theta(t+1)$.  
% \section{Recurrent Network Gradients}
% \section{Experiment details} 

% Do experiment to highlight difference between backprop and backprop with meta-learning. Experiment design: 

% x -> y1
% x -> y2

% x is the same, but ys are very different. The sequence leading up to two xs is very different and can be used to identify between y1 and y2. The meta-learning verison should be able to do far better than the non meta-learning version here. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\nocite{sutton1992adapting}
\nocite{finn2017model}
\nocite{kingma2014adam}
\nocite{rumelhart1986learning}
\nocite{werbos1974beyond}
\nocite{werbos1988generalization}
\nocite{li2017meta}
\nocite{javed2019meta}
\nocite{raghu2019rapid}
\nocite{bengio2019meta}
\nocite{williams1989learning}
\nocite{robinson1987utility}
\nocite{sutton1992adapting}
\nocite{vivek}
\nocite{hochreiter1997long}
\nocite{menick2020practical}
\nocite{tallec2017unbiased}
\nocite{cooijmans2019variance}
\nocite{sutskever2013training}
\nocite{elman1990finding}
\nocite{mikolov2009neural}
\nocite{mikolov2010recurrent}
\nocite{ollivier2015training}
\nocite{bengio1990learning}
\nocite{cho2014learning}
\bibliography{citations}

\bibliographystyle{icml2021}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
