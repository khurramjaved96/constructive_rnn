\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bengio et~al.(1990)Bengio, Bengio, and Cloutier]{bengio1990learning}
Bengio, Y., Bengio, S., and Cloutier, J.
\newblock Learning a synaptic learning rule.
\newblock Technical report, 1990.

\bibitem[Bengio et~al.(2020)Bengio, Deleu, Rahaman, Ke, Lachapelle, Bilaniuk,
  Goyal, and Pal]{bengio2019meta}
Bengio, Y., Deleu, T., Rahaman, N., Ke, R., Lachapelle, S., Bilaniuk, O.,
  Goyal, A., and Pal, C.
\newblock A meta-transfer objective for learning to disentangle causal
  mechanisms.
\newblock \emph{ICLR}, 2020.

\bibitem[Cho et~al.(2014)Cho, Van~Merri{\"e}nboer, Gulcehre, Bahdanau,
  Bougares, Schwenk, and Bengio]{cho2014learning}
Cho, K., Van~Merri{\"e}nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F.,
  Schwenk, H., and Bengio, Y.
\newblock Learning phrase representations using rnn encoder-decoder for
  statistical machine translation.
\newblock \emph{arXiv preprint arXiv:1406.1078}, 2014.

\bibitem[Cooijmans \& Martens(2019)Cooijmans and
  Martens]{cooijmans2019variance}
Cooijmans, T. and Martens, J.
\newblock On the variance of unbiased online recurrent optimization.
\newblock \emph{arXiv preprint arXiv:1902.02405}, 2019.

\bibitem[Elman(1990)]{elman1990finding}
Elman, J.~L.
\newblock Finding structure in time.
\newblock \emph{Cognitive science}, 1990.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{International conference on artificial intelligence and
  statistics}. JMLR Workshop and Conference Proceedings, 2010.

\bibitem[Glorot et~al.(2011)Glorot, Bordes, and Bengio]{glorot2011deep}
Glorot, X., Bordes, A., and Bengio, Y.
\newblock Deep sparse rectifier neural networks.
\newblock In \emph{Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}. JMLR Workshop and Conference
  Proceedings, 2011.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 1997.

\bibitem[Javed \& White(2019)Javed and White]{javed2019meta}
Javed, K. and White, M.
\newblock Meta-learning representations for continual learning.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Kapturowski et~al.(2018)Kapturowski, Ostrovski, Quan, Munos, and
  Dabney]{kapturowski2018recurrent}
Kapturowski, S., Ostrovski, G., Quan, J., Munos, R., and Dabney, W.
\newblock Recurrent experience replay in distributed reinforcement learning.
\newblock In \emph{International conference on learning representations}, 2018.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{ICLR}, 2015.

\bibitem[Li et~al.(2017)Li, Zhou, Chen, and Li]{li2017meta}
Li, Z., Zhou, F., Chen, F., and Li, H.
\newblock Meta-sgd: Learning to learn quickly for few-shot learning.
\newblock \emph{arXiv:1707.09835}, 2017.

\bibitem[Menick et~al.(2020)Menick, Elsen, Evci, Osindero, Simonyan, and
  Graves]{menick2020practical}
Menick, J., Elsen, E., Evci, U., Osindero, S., Simonyan, K., and Graves, A.
\newblock A practical sparse approximation for real time recurrent learning.
\newblock \emph{ICLR 2021}, 2020.

\bibitem[Mikolov et~al.(2009)Mikolov, Kopecky, Burget, Glembek,
  et~al.]{mikolov2009neural}
Mikolov, T., Kopecky, J., Burget, L., Glembek, O., et~al.
\newblock Neural network based language models for highly inflective languages.
\newblock In \emph{International conference on acoustics, speech and signal
  processing}, 2009.

\bibitem[Mikolov et~al.(2010)Mikolov, Karafi{\'a}t, Burget, {\v{C}}ernock{\`y},
  and Khudanpur]{mikolov2010recurrent}
Mikolov, T., Karafi{\'a}t, M., Burget, L., {\v{C}}ernock{\`y}, J., and
  Khudanpur, S.
\newblock Recurrent neural network based language model.
\newblock In \emph{International speech communication association}, 2010.

\bibitem[Mujika et~al.(2018)Mujika, Meier, and Steger]{mujika2018approximating}
Mujika, A., Meier, F., and Steger, A.
\newblock Approximating real-time recurrent learning with random kronecker
  factors.
\newblock \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Ollivier et~al.(2015)Ollivier, Tallec, and
  Charpiat]{ollivier2015training}
Ollivier, Y., Tallec, C., and Charpiat, G.
\newblock Training recurrent networks online without backtracking.
\newblock \emph{arXiv preprint arXiv:1507.07680}, 2015.

\bibitem[Rafiee et~al.(2022)Rafiee, Abbas, Ghiassian, Kumaraswamy, Sutton,
  Ludvig, and White]{animallearning}
Rafiee, B., Abbas, Z., Ghiassian, S., Kumaraswamy, R., Sutton, R., Ludvig, E.,
  and White, A.
\newblock From eye-blinks to state construction: Diagnostic benchmarks for
  online representation learning.
\newblock \emph{Adaptive Behavior}, 2022.

\bibitem[Robinson \& Fallside(1987)Robinson and Fallside]{robinson1987utility}
Robinson, A. and Fallside, F.
\newblock \emph{The utility driven dynamic error propagation network}.
\newblock University of Cambridge Department of Engineering Cambridge, MA,
  1987.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and
  Williams]{rumelhart1986learning}
Rumelhart, D.~E., Hinton, G.~E., and Williams, R.~J.
\newblock Learning representations by back-propagating errors.
\newblock \emph{Nature}, 1986.

\bibitem[Santoro et~al.(2016)Santoro, Bartunov, Botvinick, Wierstra, and
  Lillicrap]{pllr}
Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T.
\newblock Meta-learning with memory-augmented neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Schmidhuber(1987)]{schmidhuber1987evolutionary}
Schmidhuber, J.
\newblock \emph{Evolutionary principles in self-referential learning, or on
  learning how to learn: the meta-meta-... hook}.
\newblock PhD thesis, Technische Universit{\"a}t M{\"u}nchen, 1987.

\bibitem[Sutskever(2013)]{sutskever2013training}
Sutskever, I.
\newblock \emph{Training recurrent neural networks}.
\newblock University of Toronto Toronto, Canada, 2013.

\bibitem[Sutton(1992)]{sutton1992adapting}
Sutton, R.~S.
\newblock Adapting bias by gradient descent: An incremental version of
  delta-bar-delta.
\newblock In \emph{AAAI}. San Jose, CA, 1992.

\bibitem[Tallec \& Ollivier(2017)Tallec and Ollivier]{tallec2017unbiased}
Tallec, C. and Ollivier, Y.
\newblock Unbiased online recurrent optimization.
\newblock \emph{arXiv preprint arXiv:1702.05043}, 2017.

\bibitem[Tange(2011)]{Tange2011a}
Tange, O.
\newblock Gnu parallel - the command-line power tool.
\newblock \emph{;login: The USENIX Magazine}, 36\penalty0 (1):\penalty0 42--47,
  Feb 2011.
\newblock \doi{http://dx.doi.org/10.5281/zenodo.16303}.
\newblock URL \url{http://www.gnu.org/s/parallel}.

\bibitem[Veeriah et~al.(2017)Veeriah, Zhang, and Sutton]{vivek}
Veeriah, V., Zhang, S., and Sutton, R.~S.
\newblock Crossprop: Learning representations by stochastic meta-gradient
  descent in neural networks.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases},
  Cham, 2017.

\bibitem[Werbos(1974)]{werbos1974beyond}
Werbos, P.
\newblock Beyond regression:" new tools for prediction and analysis in the
  behavioral sciences.
\newblock \emph{Ph. D. dissertation, Harvard University}, 1974.

\bibitem[Werbos(1988)]{werbos1988generalization}
Werbos, P.~J.
\newblock Generalization of backpropagation with application to a recurrent gas
  market model.
\newblock \emph{Neural networks}, 1988.

\bibitem[Williams \& Peng(1990)Williams and Peng]{williams1990efficient}
Williams, R.~J. and Peng, J.
\newblock An efficient gradient-based algorithm for on-line training of
  recurrent network trajectories.
\newblock \emph{Neural computation}, 1990.

\bibitem[Williams \& Zipser(1989)Williams and Zipser]{williams1989learning}
Williams, R.~J. and Zipser, D.
\newblock A learning algorithm for continually running fully recurrent neural
  networks.
\newblock \emph{Neural computation}, 1989.

\end{thebibliography}
